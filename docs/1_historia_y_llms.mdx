---
sidebar_position: 1
---

# ¿Cómo hemos llegado hasta aquí?

## Conceptos básicos


<div style={{maxWidth: "600px"}}>
![](./img/venn.gif)
</div>


- **Inteligencia Artificial** (1956): Desarrollo de sistemas que pueden realizar tareas que normalmente requieren inteligencia humana.
    - ELIZA (1966): Chatbot que simula una conversación terapéutica mediante reglas (busca palabras clave y responde con frases predefinidas).
    - Deep Blue (1997): Computadora que derrota a Kasparov en ajedrez. Estrategias programadas y una gran capacidad de cálculo.

- **Machine Learning** o aprendizaje automático (1959): Desarrollo de algoritmos y modelos que permiten a las computadoras aprender de los datos.
    - Árboles de decisión (1960), Regresión logística (1958), K-means (1967), SVM (1995), Random Forest (2001), Gradient Boosting (2001), etc.
    - Predicciones basadas en datos (bolsa, clima, etc.)


<div style={{maxWidth: "600px"}}>
![](./img/reglas_vs_ml.jpg)
</div>


> Paradoja de  Moravec (1980): Es relativamente fácil hacer que las computadoras realicen operaciones matemáticas y lógicas, pero [es difícil hacer que realicen tareas simples que cualquier niño de 4 años puede hacer](https://www.smbc-comics.com/comic/ai-7?fbclid=IwY2xjawItKRFleHRuA2FlbQIxMAABHVZSt2Sks4COowshXz1d-2qcawJEbIr3kF3pCskfK9pFV8Oh0MgTvC1otw_aem_9YONOGize2uCs327bG33gA), como reconocer un objeto o entender un lenguaje natural.

- **Redes neuronales artificiales**: Imitan el funcionamiento de las neuronas del cerebro humano para aplicaciones de Machine Learning.
    - Neurona artificial (1943)
    - [Perceptrón](https://www.youtube.com/watch?v=l-9ALe3U-Fg) (1957)
    - Red de Hopfield (1982)
    - Backpropagation (1986)
    - [LeNet-5](https://www.youtube.com/watch?v=H0oEr40YhrQ) (1998) (reconocimiento de dígitos)

- **Deep Learning**: Subcampo de la IA que se enfoca en el desarrollo de redes neuronales profundas (redes neuronales con muchas capas ocultas) y con el se desarrolla el boom de la IA.
    - AlexNet (2012)

![](./img/deep_learning.jpg)

- **IA Generativa**: Campo de la IA que utiliza *Deep Learning* en la creación de sistemas que pueden generar contenido nuevo y original.
    - ChatGPT, Dall-E, Sora, etc.
    - Lo que hoy se ha dado en llamar **"La IA"** gracias al reciente boom de los modelos generativos.


## El Boom de la IA (del *deep learning*)

- Causas: 
    - Aumento de la capacidad de cómputo (GPUs, TPUs)
    - Disponibilidad de enormes conjuntos de datos (Internet, Big Data)
    - Avances en algoritmos y arquitecturas de redes
    - Gran incremento de la financiación e inversión industrial

- Hitos principales:
    - 2012: **AlexNet** reduce el error en **ImageNet** al 15.3% (antes 26%), demostrando el poder de las CNNs y marcando el inicio del boom.
    - 2014: **DeepFace** de Facebook alcanza precisión casi humana (97.35%) en reconocimiento facial.
    - 2014: Las **GANs** (Generative Adversarial Networks), revolucionan la generación de contenido.
    - 2016: **AlphaGo** de DeepMind derrota al campeón mundial Lee Sedol.
    - 2017: Aparece la arquitectura **Transformer**, transformando el procesamiento de lenguaje.
    - 2020: **GPT-3** de OpenAI demuestra capacidades emergentes en modelos de lenguaje a gran escala.
    - 2021: Los **modelos de difusión** (DALL-E, GLIDE) comienzan a dominar la generación de imágenes realistas.
    - 2022: **ChatGPT** (GPT-3.5) de OpenAI populariza los asistentes conversacionales.
    - 2023: **GPT-4** de OpenAI y proliferación de modelos **multimodales** (texto, imagen, audio, video).
    - 2024: Primeros modelos con capacidades avanzadas de razonamiento (**OpenAI o1**).
    - 2025: **DeepSeek-R1** (open-weights) baja el coste de los modelos de lenguaje con un rendimiento similar al de o1.


## ¿Cómo funcionan?

- De manera estadística. Aprenden a predecir la siguiente palabra basándose en las anteriores.
- No parte de cero, predice la siguiente palabra basándose en el prompt que le damos, en las anteriores (memoria) y en el contexto (configuración).
- Están entrenados con datos existentes, pero **no tienen un "conocimiento funcional" de las reglas** (por ejemplo al crear imágenes, no saben cómo funcionan las manecillas de un reloj).


### Razonadores

Los modelos razonadores dan en general un mejor resultado a costa de mayor tiempo. Integran en su funcionamiento la estrategia de cadena de pensamiento (*Chain of Thought* - CoT) habiendo sido entrenados con ejemplos de razonamiento paso a paso.
Han permitido superar nuevos niveles en *benchmarks* matemáticos y lógicos.